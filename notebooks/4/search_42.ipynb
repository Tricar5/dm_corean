{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6b7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "API_KEY = \"AIzaSyD8y8jiWkga0yw9ofVIn7SI2o4REPGxdVc\"\n",
    "q = \"\"\n",
    "# type = \"channel\"\n",
    "type = \"video\"\n",
    "# Для поиска видео с учетом даты:\n",
    "publishedAfter = '2020-01-01T00:00:00Z'\n",
    "publishedBefore = '2021-01-01T00:00:00Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a893ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "import googleapiclient.discovery as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f468de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "    \n",
    "youtube = api.build(api_service_name,\n",
    "                        api_version, \n",
    "                       developerKey = API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbbb17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e456ec88",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?part=snippet&maxResults=50&q=%D0%BA%D1%80%D0%B8%D0%BF%D1%82%D0%BE%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%BE%7CNFT%7Ccrypto+art&regionCode=RU&type=video&key=AIzaSyD8y8jiWkga0yw9ofVIn7SI2o4REPGxdVc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHttpError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16812/3662528255.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mtype\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m )\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrequest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[0mdf_supplemented\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpandas\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjson_normalize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'items'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\anaconda\\lib\\site-packages\\googleapiclient\\_helpers.py\u001B[0m in \u001B[0;36mpositional_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    129\u001B[0m                 \u001B[1;32melif\u001B[0m \u001B[0mpositional_parameters_enforcement\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0mPOSITIONAL_WARNING\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    130\u001B[0m                     \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 131\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    132\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    133\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mpositional_wrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\anaconda\\lib\\site-packages\\googleapiclient\\http.py\u001B[0m in \u001B[0;36mexecute\u001B[1;34m(self, http, num_retries)\u001B[0m\n\u001B[0;32m    935\u001B[0m             \u001B[0mcallback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    936\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mresp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstatus\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[1;36m300\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 937\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mHttpError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0muri\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muri\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    938\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpostproc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcontent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    939\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mHttpError\u001B[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?part=snippet&maxResults=50&q=%D0%BA%D1%80%D0%B8%D0%BF%D1%82%D0%BE%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%BE%7CNFT%7Ccrypto+art&regionCode=RU&type=video&key=AIzaSyD8y8jiWkga0yw9ofVIn7SI2o4REPGxdVc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "# 18.1 Тот же код, что в #6, #13 и в #16, но с вставками для экономии квоты:\n",
    "# Первый заход БЕЗ аргумента order:\n",
    "request = youtube.search().list(\n",
    "        part =\"snippet\",\n",
    "        maxResults=50,\n",
    "        q = q,\n",
    "        regionCode =\"RU\",\n",
    "        type = type\n",
    ")\n",
    "response = request.execute()  \n",
    "   \n",
    "df_supplemented = pandas.json_normalize(response['items'])\n",
    "   \n",
    "    # Цикл для прохода по всем следующим страницам с выдачей:\n",
    "i = 0\n",
    "while 'nextPageToken' in response.keys():\n",
    "    request = youtube.search().list(\n",
    "            part =\"snippet\",\n",
    "            maxResults=50,\n",
    "            q = q,\n",
    "            regionCode =\"RU\",\n",
    "            type = type,\n",
    "            pageToken = response['nextPageToken']\n",
    "        )\n",
    "    response = request.execute()  \n",
    "    \n",
    "    print(f'Итерация БЕЗ включения аргумента order №{i}')\n",
    "    i += 1\n",
    "        \n",
    "    df_additional = pandas.json_normalize(response['items'])\n",
    "    df_supplemented = pandas.concat([df_supplemented, df_additional])\n",
    "    \n",
    "    \n",
    "\n",
    "print(f\"Искомых объектов {response['pageInfo']['totalResults']}\", \\\n",
    "      f\"а найденных БЕЗ включения каких-либо значений аргумента order {len(df_supplemented.drop_duplicates(f'id.{type}Id'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e700fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Значения аргумента order:\n",
    "order_list = ['date', 'rating', 'title', 'videoCount', 'viewCount']\n",
    "order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83672d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18.2\n",
    "# Цикл для прохода по значениям аргумента order, внутри которых проход по всем страницам выдачи:\n",
    "i = 0\n",
    "for order in order_list:\n",
    "    # Для остановки алгоритма, если все искомые объекты найдены\n",
    "    # БЕЗ включения каких-либо значений аргумента order (в т.ч. вообще БЕЗ них):\n",
    "    if len(df_supplemented.drop_duplicates(f'id.{type}Id')) < response['pageInfo']['totalResults']:\n",
    "        \n",
    "        # Первый заход с каждым значение аргумента order:\n",
    "        request = youtube.search().list(\n",
    "            part =\"snippet\",\n",
    "            maxResults=50,\n",
    "            q = q,\n",
    "            regionCode =\"RU\",\n",
    "            type = type,\n",
    "            order = order\n",
    "        )\n",
    "        response = request.execute()\n",
    "    \n",
    "        # Визуализация процесса:\n",
    "        print(f'Итерация №{i}, \"order\" {order}, \"items\" {len(response[\"items\"])}')\n",
    "        i += 1\n",
    "    \n",
    "        # Занесение собранных данных в итоговую таблицу df_supplemented:\n",
    "        df = pandas.json_normalize(response[\"items\"])\n",
    "        df_supplemented = pandas.concat([df, df_supplemented])\n",
    "        \n",
    "        # Заходы с тем же значением аргумента order на следующие страницы:\n",
    "        while ('nextPageToken' in response.keys())\\\n",
    "        & (len(df_supplemented.drop_duplicates(f'id.{type}Id')) < response['pageInfo']['totalResults'])\\\n",
    "        & (len(response[\"items\"]) > 0):\n",
    "        # второе условие -- для остановки алгоритма, если все искомые объекты найдены\n",
    "        # БЕЗ какой-то из следующих страниц ( в т.ч. вообще БЕЗ них)\n",
    "        # третье условие -- для остановки алгоритма, если предыдущая страница выдачи содержит 0 объектов    \n",
    "        \n",
    "            request = youtube.search().list(\n",
    "                part =\"snippet\",\n",
    "                maxResults=50,\n",
    "                q = q,\n",
    "                regionCode =\"RU\",\n",
    "                type = type,\n",
    "                pageToken = response['nextPageToken'],\n",
    "                order = order\n",
    "            )\n",
    "            response = request.execute()  \n",
    "    \n",
    "            # Визуализация процесса:\n",
    "            print(f'Итерация №{i}, \"order\" {order}, \"items\" {len(response[\"items\"])}')\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "            # Занесение собранных данных в итоговую таблицу df_supplemented:\n",
    "            df_additional = pandas.json_normalize(response[\"items\"])\n",
    "            df_supplemented = pandas.concat([df_supplemented, df_additional])\n",
    "    \n",
    "        # Сохранение составленных для каждого значения аргумента order таблиц в Excel -- на всякий случай:\n",
    "        df_supplemented.to_excel(f'{type.capitalize()}_Sorted_by_{order}.xlsx')\n",
    "    else:\n",
    "        print('Все искомые объекты найдены БЕЗ включения некоторых значений аргумента order (в т.ч. вообще БЕЗ них)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22892f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19\n",
    "# Удаление дубликатов каналов (по их ID) и запись в Excel:\n",
    "df_supplemented.index = range(1,len(df_supplemented)+1) # сквозной индекс для итоговой таблицы\n",
    "df_supplemented = df_supplemented.drop_duplicates(f'id.{type}Id')\n",
    "display(df_supplemented)\n",
    "df_supplemented.to_excel(f'{type.capitalize()}_Not_sorted+Sorted.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee89af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supplemented = pandas.read_excel(f'{type.capitalize()}_Not_sorted+Sorted.xlsx', index_col=0)\n",
    "# возврат к выдаче без учета date; указание, что индекс в первом столбце\n",
    "df_supplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e81175",
   "metadata": {},
   "source": [
    "#19.1 Запросы БЕЗ включения аргумента order, но с учетом даты\n",
    "# Первый заход:\n",
    "request = youtube.search().list(\n",
    "        part =\"snippet\",\n",
    "        maxResults=50,\n",
    "        q = q,\n",
    "        regionCode =\"RU\",\n",
    "        type = type,\n",
    "        publishedAfter=publishedAfter,\n",
    "        publishedBefore=publishedBefore\n",
    ")\n",
    "response = request.execute()  \n",
    "   \n",
    "df_supplemented = pandas.json_normalize(response['items'])\n",
    "   \n",
    "    # Цикл для прохода по всем следующим страницам с выдачей:\n",
    "i = 0\n",
    "while 'nextPageToken' in response.keys():\n",
    "    request = youtube.search().list(\n",
    "            part =\"snippet\",\n",
    "            maxResults=50,\n",
    "            q = q,\n",
    "            regionCode =\"RU\",\n",
    "            type = type,\n",
    "            pageToken = response['nextPageToken'],\n",
    "            publishedAfter=publishedAfter,\n",
    "            publishedBefore=publishedBefore\n",
    "    )\n",
    "    response = request.execute()  \n",
    "    \n",
    "    print(f'Итерация №{i}, \"date\" {publishedAfter[:11]} - {publishedBefore[:11]}')\n",
    "    i += 1\n",
    "        \n",
    "    df_additional = pandas.json_normalize(response['items'])\n",
    "    df_supplemented = pandas.concat([df_supplemented, df_additional])\n",
    "    \n",
    "    \n",
    "\n",
    "print(f\"Искомых объектов {response['pageInfo']['totalResults']}\", \\\n",
    "      f\"а найденных БЕЗ включения каких-либо значений аргумента order {len(df_supplemented.drop_duplicates('id.channelId'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fe996",
   "metadata": {},
   "source": [
    "# 19.2\n",
    "# Цикл для прохода по значениям аргумента order, внутри которых проход по всем страницам выдачи:\n",
    "# Думаю, можно не выполнять, так как большинство включено в искомые объекты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802828ea",
   "metadata": {},
   "source": [
    "# 21 Проверка совпадения двух столбцов из выдачи:\n",
    "df_supplemented = pandas.read_excel(f'{type.capitalize()}_Not_sorted+Sorted.xlsx', index_col=0)\n",
    "# возврат к выдаче без учета date; указание, что индекс в первом столбце\n",
    "\n",
    "display(\n",
    "    df_supplemented,\n",
    "    df_supplemented['snippet.publishedAt'] == df_supplemented['snippet.publishTime'],\n",
    "    f\"Совпадений двух столбцов: {sum(df_supplemented['snippet.publishedAt'] == df_supplemented['snippet.publishTime'])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a7589",
   "metadata": {},
   "source": [
    "# 22 Проверка индексирования содержимого столбца snippet.publishedAt и строчки №1:\n",
    "df_supplemented['snippet.publishedAt'][1][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23 Lambda-функция для перезаписи в столбец snippet.publishedAt только года:\n",
    "df_supplemented['snippet.publishedAt'] = df_supplemented['snippet.publishedAt'].apply(lambda yyyy: int(yyyy[:4]))\n",
    "df_supplemented['snippet.publishedAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24 Поиск самого раннего года:\n",
    "min(df_supplemented['snippet.publishedAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fa1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25.1 Запросы БЕЗ включения аргумента order, но с учетом даты\n",
    "\n",
    "i = 0\n",
    "while year < 2021:\n",
    "     \n",
    "# Первый заход С включением аргумента date:\n",
    "    request = youtube.search().list(\n",
    "        part =\"snippet\",\n",
    "        maxResults=50,\n",
    "        q = q,\n",
    "        regionCode =\"RU\",\n",
    "        type = type,\n",
    "        publishedAfter=f'{year}-01-01T00:00:00Z'\n",
    "    )\n",
    "    response = request.execute()  \n",
    "   \n",
    "    df_additional = pandas.json_normalize(response['items'])\n",
    "    df_supplemented = pandas.concat([df_supplemented, df_additional])\n",
    "   \n",
    "    # Цикл для прохода по всем следующим страницам с выдачей:\n",
    "    while 'nextPageToken' in response.keys():\n",
    "        request = youtube.search().list(\n",
    "            part =\"snippet\",\n",
    "            maxResults=50,\n",
    "            q = q,\n",
    "            regionCode =\"RU\",\n",
    "            type = type,\n",
    "            pageToken = response['nextPageToken'],\n",
    "            publishedAfter=f'{year}-01-01T00:00:00Z'\n",
    "        )\n",
    "        response = request.execute()  \n",
    "    \n",
    "    # Визуализация процесса:\n",
    "    print(f'Итерация №{i}, \"date\" {year}')\n",
    "    i += 1\n",
    "        \n",
    "    df_additional = pandas.json_normalize(response['items'])\n",
    "    df_supplemented = pandas.concat([df_supplemented, df_additional])\n",
    "    \n",
    "\n",
    "print(f\"Искомых объектов с {year} года {response['pageInfo']['totalResults']}\", \\\n",
    "      f\"а найденных {len(df_supplemented.drop_duplicates(f'id.{type}Id'))}\")\n",
    "\n",
    "year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4af315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20\n",
    "# Удаление дубликатов каналов (по их ID) и запись в Excel:\n",
    "df_supplemented.index = range(1,len(df_supplemented)+1) # сквозной индекс для итоговой таблицы\n",
    "df_supplemented = df_supplemented.drop_duplicates(f'id.{type}Id')\n",
    "display(df_supplemented)\n",
    "df_supplemented.to_excel(f'{type.capitalize()}_Not_sorted+Sorted+Date.xlsx'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
